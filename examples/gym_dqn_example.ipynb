{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using DQN and Flashbax in gym environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/instadeepai/flashbax/blob/main/examples/gym_dqn_example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/instadeepai/flashbax.git\n",
    "!pip install ./flashbax[examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from typing import NamedTuple\n",
    "\n",
    "import haiku as hk\n",
    "import gymnasium as gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import rlax\n",
    "import chex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flashbax as fbx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Network and data classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple network function using Haiku.\n",
    "def get_network_fn(num_outputs: int):\n",
    "    \"\"\"Define a fully connected multi-layer haiku network.\"\"\"\n",
    "    def network_fn(obs: chex.Array, rng: chex.PRNGKey) -> chex.Array:\n",
    "        return hk.Sequential([  # flatten, 2x hidden + relu, output layer.\n",
    "            hk.Flatten(),\n",
    "            hk.Linear(256), jax.nn.leaky_relu,\n",
    "            hk.Linear(128), jax.nn.leaky_relu,\n",
    "            hk.Linear(num_outputs)])(obs)\n",
    "    return hk.without_apply_rng(hk.transform(network_fn))\n",
    "\n",
    "# Define a simple tuple to hold the state of the training.\n",
    "class TrainState(NamedTuple):\n",
    "    params: hk.Params\n",
    "    target_params: hk.Params\n",
    "    opt_state : optax.OptState\n",
    "\n",
    "\n",
    "# Define a simple tuple to hold the state of the environment. This is the format we will use to store transitions in our buffer.\n",
    "@chex.dataclass(frozen=True)\n",
    "class TimeStep:\n",
    "    observation: chex.Array\n",
    "    action: chex.Array\n",
    "    discount: chex.Array\n",
    "    reward: chex.Array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify our parameters \n",
    "env_id = \"CartPole-v1\"\n",
    "seed = 42\n",
    "num_envs = 1\n",
    "\n",
    "total_timesteps = 50_000\n",
    "learning_starts = 1_000\n",
    "train_frequency = 5\n",
    "target_network_frequency = 500\n",
    "sample_batch_size = 128\n",
    "buffer_size = 50_000\n",
    "tau = 1.0\n",
    "learning_rate = 1e-3\n",
    "start_e = 1.0\n",
    "end_e = 0.01\n",
    "exploration_fraction = 0.5\n",
    "gamma = 0.99\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then set up the environments\n",
    "def make_env(env_id, seed):\n",
    "    def thunk():\n",
    "        \n",
    "        env = gym.make(env_id)\n",
    "        # We use an auto reset wrapper to automatically reset the environment\n",
    "        # when the episode is done since we are using vectorized environments\n",
    "        # and we want all the environments to always be active.\n",
    "        # Additionally, we use the auto reset wrapper since we are adding transitions \n",
    "        # sequentially and we want to maintain the order in which we are adding the transitions.\n",
    "        env = gym.wrappers.AutoResetWrapper(env)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "if num_envs == 1:\n",
    "    envs = make_env(env_id, seed)()\n",
    "    assert isinstance(envs.action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "    num_actions = envs.action_space.n\n",
    "else:\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "            [make_env(env_id, seed + i) for i in range(num_envs)]\n",
    "        )\n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "    num_actions = envs.single_action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/callum/miniconda3/envs/flashbax/lib/python3.9/site-packages/flashbax/buffers/trajectory_buffer.py:471: UserWarning: Setting max_size dynamically sets the `max_length_time_axis` to be `max_size`//`add_batch_size = 50000`.This allows one to control exactly how many transitions are stored in the buffer.Note that this overrides the `max_length_time_axis` argument.\n",
      "  warnings.warn(\n",
      "/Users/callum/miniconda3/envs/flashbax/lib/python3.9/site-packages/flashbax/buffers/trajectory_buffer.py:138: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead.\n",
      "  chex.assert_axis_dimension_lteq(jax.tree_leaves(batch)[0], 1, max_length_time_axis)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Current Training Step: 0\n",
      "Current Training Step: 1000\n",
      "SPS: 1632\n",
      "Loss 0.28701562\n",
      "SPS: 1731\n",
      "Loss 0.15886527\n",
      "SPS: 1826\n",
      "Loss 0.04291299\n",
      "SPS: 1917\n",
      "Loss 0.013063445\n",
      "SPS: 2002\n",
      "Loss 0.007482495\n",
      "SPS: 1906\n",
      "Loss 0.10549195\n",
      "SPS: 1980\n",
      "Loss 0.044599906\n",
      "SPS: 2049\n",
      "Loss 0.033348784\n",
      "SPS: 2114\n",
      "Loss 0.07498935\n",
      "Current Training Step: 2000\n",
      "SPS: 2176\n",
      "Loss 0.079916604\n",
      "SPS: 2234\n",
      "Loss 0.099185415\n",
      "SPS: 2293\n",
      "Loss 0.09092561\n",
      "SPS: 2350\n",
      "Loss 0.16927493\n",
      "SPS: 2405\n",
      "Loss 0.097453564\n",
      "SPS: 2458\n",
      "Loss 0.10643174\n",
      "SPS: 2506\n",
      "Loss 0.28333086\n",
      "SPS: 2555\n",
      "Loss 0.1799488\n",
      "SPS: 2601\n",
      "Loss 0.1939533\n",
      "SPS: 2645\n",
      "Loss 0.17877991\n",
      "Current Training Step: 3000\n",
      "SPS: 2685\n",
      "Loss 0.1255859\n",
      "SPS: 2724\n",
      "Loss 0.27233487\n",
      "SPS: 2764\n",
      "Loss 0.25333983\n",
      "SPS: 2801\n",
      "Loss 0.15252432\n",
      "SPS: 2840\n",
      "Loss 0.2792627\n",
      "SPS: 2877\n",
      "Loss 0.15868285\n",
      "SPS: 2911\n",
      "Loss 0.33359522\n",
      "SPS: 2945\n",
      "Loss 0.16947365\n",
      "SPS: 2978\n",
      "Loss 0.3194501\n",
      "SPS: 3009\n",
      "Loss 0.34927875\n",
      "Current Training Step: 4000\n",
      "SPS: 3037\n",
      "Loss 0.44799635\n",
      "SPS: 3063\n",
      "Loss 0.35320124\n",
      "SPS: 3088\n",
      "Loss 0.28368106\n",
      "SPS: 3115\n",
      "Loss 0.22841756\n",
      "SPS: 3143\n",
      "Loss 0.16921397\n",
      "SPS: 3168\n",
      "Loss 0.12916036\n",
      "SPS: 3190\n",
      "Loss 1.020685\n",
      "SPS: 3215\n",
      "Loss 0.38562238\n",
      "SPS: 3236\n",
      "Loss 0.4781611\n",
      "SPS: 3257\n",
      "Loss 0.32129854\n",
      "Current Training Step: 5000\n",
      "SPS: 3278\n",
      "Loss 0.23321068\n",
      "SPS: 3296\n",
      "Loss 0.4429329\n",
      "SPS: 3318\n",
      "Loss 0.28320792\n",
      "SPS: 3338\n",
      "Loss 1.1642067\n",
      "SPS: 3358\n",
      "Loss 0.14088956\n",
      "SPS: 3378\n",
      "Loss 0.1747345\n",
      "SPS: 3394\n",
      "Loss 0.23608509\n",
      "SPS: 3414\n",
      "Loss 0.45430937\n",
      "SPS: 3432\n",
      "Loss 0.33316538\n",
      "SPS: 3449\n",
      "Loss 0.4836108\n",
      "Current Training Step: 6000\n",
      "SPS: 3466\n",
      "Loss 0.4095198\n",
      "SPS: 3480\n",
      "Loss 1.0780164\n",
      "SPS: 3497\n",
      "Loss 0.29846972\n",
      "SPS: 3512\n",
      "Loss 1.1669204\n",
      "SPS: 3529\n",
      "Loss 0.38812605\n",
      "SPS: 3544\n",
      "Loss 0.37976336\n",
      "SPS: 3557\n",
      "Loss 0.6025654\n",
      "SPS: 3571\n",
      "Loss 1.0381615\n",
      "SPS: 3585\n",
      "Loss 0.23508923\n",
      "SPS: 3601\n",
      "Loss 0.92410636\n",
      "Current Training Step: 7000\n",
      "SPS: 3610\n",
      "Loss 0.18334757\n",
      "SPS: 3622\n",
      "Loss 0.6835128\n",
      "SPS: 3635\n",
      "Loss 0.5196424\n",
      "SPS: 3646\n",
      "Loss 0.75306463\n",
      "SPS: 3658\n",
      "Loss 0.62269384\n",
      "SPS: 3669\n",
      "Loss 0.7257873\n",
      "SPS: 3681\n",
      "Loss 0.43891743\n",
      "SPS: 3693\n",
      "Loss 1.0479631\n",
      "SPS: 3700\n",
      "Loss 0.96378684\n",
      "SPS: 3712\n",
      "Loss 0.8430784\n",
      "Current Training Step: 8000\n",
      "SPS: 3722\n",
      "Loss 0.3542033\n",
      "SPS: 3731\n",
      "Loss 0.85706544\n",
      "SPS: 3737\n",
      "Loss 0.2987731\n",
      "SPS: 3747\n",
      "Loss 0.30506968\n",
      "SPS: 3758\n",
      "Loss 1.410465\n",
      "SPS: 3763\n",
      "Loss 0.6550817\n",
      "SPS: 3768\n",
      "Loss 2.0366373\n",
      "SPS: 3769\n",
      "Loss 0.80135\n",
      "SPS: 3769\n",
      "Loss 1.1704073\n",
      "SPS: 3777\n",
      "Loss 0.3337914\n",
      "Current Training Step: 9000\n",
      "SPS: 3783\n",
      "Loss 1.5868068\n",
      "SPS: 3791\n",
      "Loss 0.54114294\n",
      "SPS: 3801\n",
      "Loss 1.5432749\n",
      "SPS: 3808\n",
      "Loss 0.47805744\n",
      "SPS: 3817\n",
      "Loss 0.6359874\n",
      "SPS: 3824\n",
      "Loss 0.36474246\n",
      "SPS: 3826\n",
      "Loss 2.1675272\n",
      "SPS: 3825\n",
      "Loss 0.32114035\n",
      "SPS: 3826\n",
      "Loss 0.8733313\n",
      "SPS: 3822\n",
      "Loss 0.5874443\n",
      "Current Training Step: 10000\n",
      "SPS: 3815\n",
      "Loss 0.723191\n",
      "SPS: 3817\n",
      "Loss 0.6260694\n",
      "SPS: 3824\n",
      "Loss 2.279556\n",
      "SPS: 3832\n",
      "Loss 1.1422163\n",
      "SPS: 3839\n",
      "Loss 0.29216394\n",
      "SPS: 3847\n",
      "Loss 0.5833337\n",
      "SPS: 3854\n",
      "Loss 3.7161028\n",
      "SPS: 3862\n",
      "Loss 1.5186989\n",
      "SPS: 3867\n",
      "Loss 2.9564524\n",
      "SPS: 3875\n",
      "Loss 0.7792301\n",
      "Current Training Step: 11000\n",
      "SPS: 3882\n",
      "Loss 0.9897866\n",
      "SPS: 3890\n",
      "Loss 0.51391774\n",
      "SPS: 3896\n",
      "Loss 1.0931096\n",
      "SPS: 3903\n",
      "Loss 1.2171127\n",
      "SPS: 3909\n",
      "Loss 1.0330976\n",
      "SPS: 3909\n",
      "Loss 1.0392166\n",
      "SPS: 3906\n",
      "Loss 0.6983172\n",
      "SPS: 3903\n",
      "Loss 2.1848464\n",
      "SPS: 3909\n",
      "Loss 2.5216355\n",
      "SPS: 3915\n",
      "Loss 1.0542393\n",
      "Current Training Step: 12000\n",
      "SPS: 3920\n",
      "Loss 3.9662077\n",
      "SPS: 3925\n",
      "Loss 2.2881956\n",
      "SPS: 3932\n",
      "Loss 0.6901476\n",
      "SPS: 3937\n",
      "Loss 5.154683\n",
      "SPS: 3942\n",
      "Loss 2.3920634\n",
      "SPS: 3947\n",
      "Loss 0.76471704\n",
      "SPS: 3953\n",
      "Loss 7.7535696\n",
      "SPS: 3958\n",
      "Loss 2.5458374\n",
      "SPS: 3961\n",
      "Loss 3.3054016\n",
      "SPS: 3967\n",
      "Loss 1.6022314\n",
      "Current Training Step: 13000\n",
      "SPS: 3970\n",
      "Loss 1.122114\n",
      "SPS: 3974\n",
      "Loss 3.2383823\n",
      "SPS: 3978\n",
      "Loss 5.7995563\n",
      "SPS: 3983\n",
      "Loss 1.2571843\n",
      "SPS: 3988\n",
      "Loss 0.97322303\n",
      "SPS: 3990\n",
      "Loss 1.156596\n",
      "SPS: 3991\n",
      "Loss 3.4012575\n",
      "SPS: 3995\n",
      "Loss 1.2870454\n",
      "SPS: 3998\n",
      "Loss 1.8736128\n",
      "SPS: 4001\n",
      "Loss 2.5589948\n",
      "Current Training Step: 14000\n",
      "SPS: 4005\n",
      "Loss 0.634744\n",
      "SPS: 4008\n",
      "Loss 1.9363413\n",
      "SPS: 4012\n",
      "Loss 1.9324061\n",
      "SPS: 4014\n",
      "Loss 1.2160056\n",
      "SPS: 4008\n",
      "Loss 7.076639\n",
      "SPS: 4005\n",
      "Loss 0.63813806\n",
      "SPS: 4003\n",
      "Loss 2.8211184\n",
      "SPS: 4000\n",
      "Loss 3.2991285\n",
      "SPS: 4003\n",
      "Loss 4.605053\n",
      "SPS: 4007\n",
      "Loss 1.9848211\n",
      "Current Training Step: 15000\n",
      "SPS: 4010\n",
      "Loss 0.95830274\n",
      "SPS: 4011\n",
      "Loss 0.4156963\n",
      "SPS: 4009\n",
      "Loss 2.4669957\n",
      "SPS: 4012\n",
      "Loss 1.461868\n",
      "SPS: 4015\n",
      "Loss 5.9580727\n",
      "SPS: 4018\n",
      "Loss 1.1639127\n",
      "SPS: 4020\n",
      "Loss 3.8359048\n",
      "SPS: 4023\n",
      "Loss 2.4145985\n",
      "SPS: 4026\n",
      "Loss 2.1086454\n",
      "SPS: 4029\n",
      "Loss 1.0677619\n",
      "Current Training Step: 16000\n",
      "SPS: 4031\n",
      "Loss 2.3410158\n",
      "SPS: 4033\n",
      "Loss 1.5785546\n",
      "SPS: 4036\n",
      "Loss 2.0439165\n",
      "SPS: 4039\n",
      "Loss 2.3548965\n",
      "SPS: 4041\n",
      "Loss 7.5612516\n",
      "SPS: 4040\n",
      "Loss 1.6532857\n",
      "SPS: 4037\n",
      "Loss 1.441661\n",
      "SPS: 4037\n",
      "Loss 1.7034266\n",
      "SPS: 4037\n",
      "Loss 0.9491638\n",
      "SPS: 4038\n",
      "Loss 1.7996914\n",
      "Current Training Step: 17000\n",
      "SPS: 4040\n",
      "Loss 8.670707\n",
      "SPS: 4042\n",
      "Loss 1.8144614\n",
      "SPS: 4045\n",
      "Loss 0.6301992\n",
      "SPS: 4048\n",
      "Loss 1.9070222\n",
      "SPS: 4045\n",
      "Loss 4.8824396\n",
      "SPS: 4046\n",
      "Loss 5.9733515\n",
      "SPS: 4047\n",
      "Loss 1.1541252\n",
      "SPS: 4049\n",
      "Loss 7.2729974\n",
      "SPS: 4052\n",
      "Loss 1.3939384\n",
      "SPS: 4054\n",
      "Loss 1.3373849\n",
      "Current Training Step: 18000\n",
      "SPS: 4056\n",
      "Loss 4.212116\n",
      "SPS: 4058\n",
      "Loss 2.9282491\n",
      "SPS: 4060\n",
      "Loss 3.2474346\n",
      "SPS: 4062\n",
      "Loss 6.602314\n",
      "SPS: 4063\n",
      "Loss 2.3405452\n",
      "SPS: 4065\n",
      "Loss 1.0899631\n",
      "SPS: 4067\n",
      "Loss 2.6761107\n",
      "SPS: 4069\n",
      "Loss 1.824478\n",
      "SPS: 4070\n",
      "Loss 1.2756078\n",
      "SPS: 4072\n",
      "Loss 1.3122628\n",
      "Current Training Step: 19000\n",
      "SPS: 4075\n",
      "Loss 2.6807895\n",
      "SPS: 4076\n",
      "Loss 2.1552038\n",
      "SPS: 4077\n",
      "Loss 1.8927789\n",
      "SPS: 4079\n",
      "Loss 6.8625865\n",
      "SPS: 4081\n",
      "Loss 4.564766\n",
      "SPS: 4082\n",
      "Loss 1.2308376\n",
      "SPS: 4083\n",
      "Loss 1.939095\n",
      "SPS: 4085\n",
      "Loss 1.1113764\n",
      "SPS: 4087\n",
      "Loss 5.428403\n",
      "SPS: 4088\n",
      "Loss 2.0808952\n",
      "Current Training Step: 20000\n",
      "SPS: 4090\n",
      "Loss 6.047923\n",
      "SPS: 4090\n",
      "Loss 1.4206617\n",
      "SPS: 4090\n",
      "Loss 9.76038\n",
      "SPS: 4088\n",
      "Loss 3.726622\n",
      "SPS: 4089\n",
      "Loss 7.0955667\n",
      "SPS: 4090\n",
      "Loss 1.4617863\n",
      "SPS: 4091\n",
      "Loss 5.902463\n",
      "SPS: 4093\n",
      "Loss 1.1553144\n",
      "SPS: 4095\n",
      "Loss 0.74474216\n",
      "SPS: 4096\n",
      "Loss 1.3984538\n",
      "Current Training Step: 21000\n",
      "SPS: 4097\n",
      "Loss 9.831293\n",
      "SPS: 4097\n",
      "Loss 1.0785707\n",
      "SPS: 4098\n",
      "Loss 1.0341165\n",
      "SPS: 4099\n",
      "Loss 1.2007004\n",
      "SPS: 4100\n",
      "Loss 7.47928\n",
      "SPS: 4101\n",
      "Loss 2.7814944\n",
      "SPS: 4102\n",
      "Loss 8.114712\n",
      "SPS: 4104\n",
      "Loss 1.5810356\n",
      "SPS: 4105\n",
      "Loss 1.4308156\n",
      "SPS: 4104\n",
      "Loss 2.068032\n",
      "Current Training Step: 22000\n",
      "SPS: 4103\n",
      "Loss 1.899984\n",
      "SPS: 4105\n",
      "Loss 1.7134696\n",
      "SPS: 4106\n",
      "Loss 11.48358\n",
      "SPS: 4107\n",
      "Loss 1.1424644\n",
      "SPS: 4109\n",
      "Loss 5.514695\n",
      "SPS: 4110\n",
      "Loss 5.3594184\n",
      "SPS: 4112\n",
      "Loss 2.1907349\n",
      "SPS: 4113\n",
      "Loss 1.2444237\n",
      "SPS: 4112\n",
      "Loss 3.594558\n",
      "SPS: 4112\n",
      "Loss 5.677954\n",
      "Current Training Step: 23000\n",
      "SPS: 4114\n",
      "Loss 1.7018816\n",
      "SPS: 4115\n",
      "Loss 2.6567996\n",
      "SPS: 4117\n",
      "Loss 2.722796\n",
      "SPS: 4118\n",
      "Loss 2.9477952\n",
      "SPS: 4120\n",
      "Loss 12.912895\n",
      "SPS: 4122\n",
      "Loss 3.24118\n",
      "SPS: 4119\n",
      "Loss 9.6728115\n",
      "SPS: 4118\n",
      "Loss 2.0892591\n",
      "SPS: 4117\n",
      "Loss 1.4731232\n",
      "SPS: 4118\n",
      "Loss 3.3311386\n",
      "Current Training Step: 24000\n",
      "SPS: 4119\n",
      "Loss 1.1247314\n",
      "SPS: 4120\n",
      "Loss 6.909281\n",
      "SPS: 4116\n",
      "Loss 0.99908143\n",
      "SPS: 4116\n",
      "Loss 6.3455057\n",
      "SPS: 4116\n",
      "Loss 1.2949846\n",
      "SPS: 4116\n",
      "Loss 2.2424645\n",
      "SPS: 4116\n",
      "Loss 3.2372212\n",
      "SPS: 4117\n",
      "Loss 1.7794302\n",
      "SPS: 4117\n",
      "Loss 7.0375137\n",
      "SPS: 4117\n",
      "Loss 1.2908823\n",
      "Current Training Step: 25000\n",
      "SPS: 4118\n",
      "Loss 1.36741\n",
      "SPS: 4119\n",
      "Loss 0.8939357\n",
      "SPS: 4119\n",
      "Loss 1.5517235\n",
      "SPS: 4120\n",
      "Loss 3.6230986\n",
      "SPS: 4121\n",
      "Loss 0.5753411\n",
      "SPS: 4122\n",
      "Loss 4.1470585\n",
      "SPS: 4121\n",
      "Loss 2.9253595\n",
      "SPS: 4122\n",
      "Loss 4.522478\n",
      "SPS: 4122\n",
      "Loss 7.6789794\n",
      "SPS: 4123\n",
      "Loss 4.076322\n",
      "Current Training Step: 26000\n",
      "SPS: 4124\n",
      "Loss 3.3293579\n",
      "SPS: 4125\n",
      "Loss 1.6513481\n",
      "SPS: 4125\n",
      "Loss 2.3557649\n",
      "SPS: 4125\n",
      "Loss 1.0191879\n",
      "SPS: 4126\n",
      "Loss 2.8388355\n",
      "SPS: 4126\n",
      "Loss 4.508791\n",
      "SPS: 4127\n",
      "Loss 0.7814158\n",
      "SPS: 4129\n",
      "Loss 2.4212484\n",
      "SPS: 4128\n",
      "Loss 9.359814\n",
      "SPS: 4129\n",
      "Loss 11.063937\n",
      "Current Training Step: 27000\n",
      "SPS: 4130\n",
      "Loss 10.191163\n",
      "SPS: 4131\n",
      "Loss 1.7189072\n",
      "SPS: 4132\n",
      "Loss 1.8500315\n",
      "SPS: 4133\n",
      "Loss 3.7846851\n",
      "SPS: 4134\n",
      "Loss 2.951565\n",
      "SPS: 4135\n",
      "Loss 3.2930098\n",
      "SPS: 4135\n",
      "Loss 8.526975\n",
      "SPS: 4136\n",
      "Loss 1.5512383\n",
      "SPS: 4137\n",
      "Loss 1.2622173\n",
      "SPS: 4137\n",
      "Loss 11.55518\n",
      "Current Training Step: 28000\n",
      "SPS: 4139\n",
      "Loss 1.3610923\n",
      "SPS: 4139\n",
      "Loss 4.002864\n",
      "SPS: 4140\n",
      "Loss 3.2590618\n",
      "SPS: 4140\n",
      "Loss 1.606164\n",
      "SPS: 4141\n",
      "Loss 1.6873443\n",
      "SPS: 4142\n",
      "Loss 1.7338185\n",
      "SPS: 4143\n",
      "Loss 4.5576735\n",
      "SPS: 4144\n",
      "Loss 1.6921347\n",
      "SPS: 4145\n",
      "Loss 10.182474\n",
      "SPS: 4145\n",
      "Loss 1.6066287\n",
      "Current Training Step: 29000\n",
      "SPS: 4146\n",
      "Loss 1.5418383\n",
      "SPS: 4146\n",
      "Loss 2.0374894\n",
      "SPS: 4147\n",
      "Loss 3.5455046\n",
      "SPS: 4148\n",
      "Loss 2.6503594\n",
      "SPS: 4149\n",
      "Loss 1.2928793\n",
      "SPS: 4150\n",
      "Loss 1.6148884\n",
      "SPS: 4150\n",
      "Loss 1.4531631\n",
      "SPS: 4151\n",
      "Loss 2.3592753\n",
      "SPS: 4152\n",
      "Loss 4.1914515\n",
      "SPS: 4153\n",
      "Loss 6.832278\n",
      "Current Training Step: 30000\n",
      "SPS: 4154\n",
      "Loss 1.2536879\n",
      "SPS: 4154\n",
      "Loss 8.650646\n",
      "SPS: 4155\n",
      "Loss 0.5894225\n",
      "SPS: 4155\n",
      "Loss 1.8825963\n",
      "SPS: 4156\n",
      "Loss 2.0170093\n",
      "SPS: 4155\n",
      "Loss 2.2085032\n",
      "SPS: 4156\n",
      "Loss 4.2259927\n",
      "SPS: 4157\n",
      "Loss 1.0512131\n",
      "SPS: 4157\n",
      "Loss 22.939144\n",
      "SPS: 4158\n",
      "Loss 1.9862816\n",
      "Current Training Step: 31000\n",
      "SPS: 4158\n",
      "Loss 1.9479402\n",
      "SPS: 4158\n",
      "Loss 8.065892\n",
      "SPS: 4159\n",
      "Loss 1.4317507\n",
      "SPS: 4160\n",
      "Loss 4.748575\n",
      "SPS: 4161\n",
      "Loss 0.7654196\n",
      "SPS: 4161\n",
      "Loss 1.4270642\n",
      "SPS: 4162\n",
      "Loss 12.297194\n",
      "SPS: 4162\n",
      "Loss 19.242035\n",
      "SPS: 4163\n",
      "Loss 3.6963\n",
      "SPS: 4163\n",
      "Loss 1.6000772\n",
      "Current Training Step: 32000\n",
      "SPS: 4164\n",
      "Loss 1.9598843\n",
      "SPS: 4164\n",
      "Loss 1.9583712\n",
      "SPS: 4165\n",
      "Loss 1.689841\n",
      "SPS: 4165\n",
      "Loss 1.0590744\n",
      "SPS: 4166\n",
      "Loss 1.2840803\n",
      "SPS: 4167\n",
      "Loss 19.094471\n",
      "SPS: 4167\n",
      "Loss 1.5862455\n",
      "SPS: 4168\n",
      "Loss 2.8298159\n",
      "SPS: 4168\n",
      "Loss 0.8588623\n",
      "SPS: 4169\n",
      "Loss 2.97953\n",
      "Current Training Step: 33000\n",
      "SPS: 4169\n",
      "Loss 1.3297284\n",
      "SPS: 4170\n",
      "Loss 3.4996994\n",
      "SPS: 4170\n",
      "Loss 15.07666\n",
      "SPS: 4170\n",
      "Loss 1.0171752\n",
      "SPS: 4171\n",
      "Loss 1.8206558\n",
      "SPS: 4171\n",
      "Loss 2.596973\n",
      "SPS: 4172\n",
      "Loss 1.3549865\n",
      "SPS: 4172\n",
      "Loss 2.3639374\n",
      "SPS: 4172\n",
      "Loss 1.9293082\n",
      "SPS: 4173\n",
      "Loss 0.91103774\n",
      "Current Training Step: 34000\n",
      "SPS: 4174\n",
      "Loss 18.884325\n",
      "SPS: 4174\n",
      "Loss 2.3804536\n",
      "SPS: 4175\n",
      "Loss 7.45949\n",
      "SPS: 4175\n",
      "Loss 1.0204642\n",
      "SPS: 4175\n",
      "Loss 1.3936052\n",
      "SPS: 4176\n",
      "Loss 3.0616672\n",
      "SPS: 4176\n",
      "Loss 10.110337\n",
      "SPS: 4176\n",
      "Loss 0.7678531\n",
      "SPS: 4177\n",
      "Loss 0.83604956\n",
      "SPS: 4177\n",
      "Loss 1.5977854\n",
      "Current Training Step: 35000\n",
      "SPS: 4178\n",
      "Loss 1.8300785\n",
      "SPS: 4178\n",
      "Loss 26.689236\n",
      "SPS: 4179\n",
      "Loss 1.6534297\n",
      "SPS: 4179\n",
      "Loss 3.0792112\n",
      "SPS: 4180\n",
      "Loss 2.0347872\n",
      "SPS: 4180\n",
      "Loss 10.66244\n",
      "SPS: 4180\n",
      "Loss 2.354067\n",
      "SPS: 4181\n",
      "Loss 1.4221911\n",
      "SPS: 4181\n",
      "Loss 2.1693072\n",
      "SPS: 4182\n",
      "Loss 2.3616872\n",
      "Current Training Step: 36000\n",
      "SPS: 4182\n",
      "Loss 0.5282694\n",
      "SPS: 4183\n",
      "Loss 2.2193558\n",
      "SPS: 4184\n",
      "Loss 12.873274\n",
      "SPS: 4184\n",
      "Loss 1.387526\n",
      "SPS: 4184\n",
      "Loss 1.2725364\n",
      "SPS: 4185\n",
      "Loss 1.718086\n",
      "SPS: 4185\n",
      "Loss 2.2418616\n",
      "SPS: 4186\n",
      "Loss 0.9406799\n",
      "SPS: 4186\n",
      "Loss 3.6894307\n",
      "SPS: 4187\n",
      "Loss 0.5709201\n",
      "Current Training Step: 37000\n",
      "SPS: 4187\n",
      "Loss 2.371714\n",
      "SPS: 4187\n",
      "Loss 9.764476\n",
      "SPS: 4187\n",
      "Loss 0.66514325\n",
      "SPS: 4187\n",
      "Loss 1.9513823\n",
      "SPS: 4188\n",
      "Loss 3.597036\n",
      "SPS: 4188\n",
      "Loss 2.076119\n",
      "SPS: 4188\n",
      "Loss 1.0351213\n",
      "SPS: 4189\n",
      "Loss 4.4350643\n",
      "SPS: 4189\n",
      "Loss 0.6323916\n",
      "SPS: 4189\n",
      "Loss 1.7804317\n",
      "Current Training Step: 38000\n",
      "SPS: 4190\n",
      "Loss 1.385396\n",
      "SPS: 4190\n",
      "Loss 1.329544\n",
      "SPS: 4190\n",
      "Loss 2.334295\n",
      "SPS: 4191\n",
      "Loss 9.006535\n",
      "SPS: 4191\n",
      "Loss 2.6263351\n",
      "SPS: 4192\n",
      "Loss 1.2517424\n",
      "SPS: 4192\n",
      "Loss 25.37254\n",
      "SPS: 4193\n",
      "Loss 2.688538\n",
      "SPS: 4193\n",
      "Loss 1.558408\n",
      "SPS: 4193\n",
      "Loss 3.4699647\n",
      "Current Training Step: 39000\n",
      "SPS: 4193\n",
      "Loss 1.2012545\n",
      "SPS: 4194\n",
      "Loss 1.103453\n",
      "SPS: 4194\n",
      "Loss 4.3175516\n",
      "SPS: 4194\n",
      "Loss 0.7963848\n",
      "SPS: 4195\n",
      "Loss 1.8267884\n",
      "SPS: 4195\n",
      "Loss 2.170798\n",
      "SPS: 4195\n",
      "Loss 1.6012055\n",
      "SPS: 4196\n",
      "Loss 1.8353813\n",
      "SPS: 4196\n",
      "Loss 5.1197777\n",
      "SPS: 4196\n",
      "Loss 0.6411294\n",
      "Current Training Step: 40000\n",
      "SPS: 4196\n",
      "Loss 2.7587364\n",
      "SPS: 4197\n",
      "Loss 2.3763304\n",
      "SPS: 4197\n",
      "Loss 1.3188909\n",
      "SPS: 4197\n",
      "Loss 6.0846395\n",
      "SPS: 4197\n",
      "Loss 1.2207408\n",
      "SPS: 4198\n",
      "Loss 32.369064\n",
      "SPS: 4198\n",
      "Loss 2.4896514\n",
      "SPS: 4198\n",
      "Loss 1.3832443\n",
      "SPS: 4199\n",
      "Loss 1.5824062\n",
      "SPS: 4198\n",
      "Loss 0.9096811\n",
      "Current Training Step: 41000\n",
      "SPS: 4199\n",
      "Loss 10.668706\n",
      "SPS: 4199\n",
      "Loss 1.8984153\n",
      "SPS: 4199\n",
      "Loss 10.877946\n",
      "SPS: 4199\n",
      "Loss 2.0839887\n",
      "SPS: 4200\n",
      "Loss 3.827212\n",
      "SPS: 4200\n",
      "Loss 21.58629\n",
      "SPS: 4201\n",
      "Loss 2.5234854\n",
      "SPS: 4201\n",
      "Loss 2.652581\n",
      "SPS: 4201\n",
      "Loss 1.345547\n",
      "SPS: 4201\n",
      "Loss 1.7076213\n",
      "Current Training Step: 42000\n",
      "SPS: 4201\n",
      "Loss 1.5039532\n",
      "SPS: 4202\n",
      "Loss 1.2039696\n",
      "SPS: 4202\n",
      "Loss 12.524965\n",
      "SPS: 4202\n",
      "Loss 1.123028\n",
      "SPS: 4202\n",
      "Loss 1.905533\n",
      "SPS: 4203\n",
      "Loss 1.9157236\n",
      "SPS: 4203\n",
      "Loss 0.7728113\n",
      "SPS: 4203\n",
      "Loss 3.1018984\n",
      "SPS: 4203\n",
      "Loss 3.451577\n",
      "SPS: 4204\n",
      "Loss 1.1814361\n",
      "Current Training Step: 43000\n",
      "SPS: 4204\n",
      "Loss 1.560941\n",
      "SPS: 4204\n",
      "Loss 0.714478\n",
      "SPS: 4205\n",
      "Loss 0.9756636\n",
      "SPS: 4205\n",
      "Loss 3.5388412\n",
      "SPS: 4206\n",
      "Loss 1.8825091\n",
      "SPS: 4206\n",
      "Loss 0.48448902\n",
      "SPS: 4206\n",
      "Loss 2.4159431\n",
      "SPS: 4206\n",
      "Loss 1.671765\n",
      "SPS: 4207\n",
      "Loss 2.6061711\n",
      "SPS: 4207\n",
      "Loss 1.91163\n",
      "Current Training Step: 44000\n",
      "SPS: 4207\n",
      "Loss 1.3245279\n",
      "SPS: 4207\n",
      "Loss 0.650567\n",
      "SPS: 4208\n",
      "Loss 2.5621483\n",
      "SPS: 4208\n",
      "Loss 2.876633\n",
      "SPS: 4208\n",
      "Loss 1.4705718\n",
      "SPS: 4209\n",
      "Loss 3.8507872\n",
      "SPS: 4209\n",
      "Loss 2.8408487\n",
      "SPS: 4209\n",
      "Loss 1.4331051\n",
      "SPS: 4209\n",
      "Loss 1.4194415\n",
      "SPS: 4210\n",
      "Loss 0.89073485\n",
      "Current Training Step: 45000\n",
      "SPS: 4210\n",
      "Loss 2.1371293\n",
      "SPS: 4210\n",
      "Loss 6.3472843\n",
      "SPS: 4210\n",
      "Loss 1.2440863\n",
      "SPS: 4210\n",
      "Loss 2.1700644\n",
      "SPS: 4210\n",
      "Loss 5.760385\n",
      "SPS: 4211\n",
      "Loss 1.3776145\n",
      "SPS: 4211\n",
      "Loss 2.2250075\n",
      "SPS: 4211\n",
      "Loss 7.0649343\n",
      "SPS: 4211\n",
      "Loss 1.3747331\n",
      "SPS: 4211\n",
      "Loss 1.3226588\n",
      "Current Training Step: 46000\n",
      "SPS: 4212\n",
      "Loss 10.270001\n",
      "SPS: 4212\n",
      "Loss 1.1179013\n",
      "SPS: 4212\n",
      "Loss 1.7460755\n",
      "SPS: 4212\n",
      "Loss 0.7038499\n",
      "SPS: 4213\n",
      "Loss 1.7515948\n",
      "SPS: 4213\n",
      "Loss 16.990005\n",
      "SPS: 4213\n",
      "Loss 1.2726767\n",
      "SPS: 4213\n",
      "Loss 1.1395807\n",
      "SPS: 4214\n",
      "Loss 7.024561\n",
      "SPS: 4214\n",
      "Loss 2.0856066\n",
      "Current Training Step: 47000\n",
      "SPS: 4214\n",
      "Loss 3.8131433\n",
      "SPS: 4215\n",
      "Loss 1.0456439\n",
      "SPS: 4214\n",
      "Loss 2.19875\n",
      "SPS: 4214\n",
      "Loss 2.2129793\n",
      "SPS: 4215\n",
      "Loss 0.65633756\n",
      "SPS: 4215\n",
      "Loss 1.1259266\n",
      "SPS: 4215\n",
      "Loss 19.353945\n",
      "SPS: 4216\n",
      "Loss 2.678985\n",
      "SPS: 4216\n",
      "Loss 2.0083547\n",
      "SPS: 4216\n",
      "Loss 1.0588558\n",
      "Current Training Step: 48000\n",
      "SPS: 4216\n",
      "Loss 24.099636\n",
      "SPS: 4216\n",
      "Loss 3.6010191\n",
      "SPS: 4217\n",
      "Loss 12.934091\n",
      "SPS: 4217\n",
      "Loss 2.681994\n",
      "SPS: 4217\n",
      "Loss 1.0519726\n",
      "SPS: 4218\n",
      "Loss 7.450323\n",
      "SPS: 4218\n",
      "Loss 1.0415592\n",
      "SPS: 4218\n",
      "Loss 1.2744045\n",
      "SPS: 4219\n",
      "Loss 2.9278054\n",
      "SPS: 4219\n",
      "Loss 17.35235\n",
      "Current Training Step: 49000\n",
      "SPS: 4219\n",
      "Loss 0.9944161\n",
      "SPS: 4207\n",
      "Loss 1.9299394\n",
      "SPS: 4208\n",
      "Loss 4.4711056\n",
      "SPS: 4208\n",
      "Loss 34.297394\n",
      "SPS: 4208\n",
      "Loss 2.8612108\n",
      "SPS: 4208\n",
      "Loss 2.6975265\n",
      "SPS: 4209\n",
      "Loss 1.3237376\n",
      "SPS: 4209\n",
      "Loss 1.6212702\n",
      "SPS: 4209\n",
      "Loss 1.526588\n",
      "SPS: 4209\n",
      "Loss 16.263725\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "with jax.default_device(jax.devices('cpu')[0]):\n",
    "\n",
    "    # Specify the random seeds we will use for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key, q_key = jax.random.split(key, 2)\n",
    "\n",
    "    # Set up the network and optimiser\n",
    "    q_network = get_network_fn(num_actions)\n",
    "    optim = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Get an initial observation from the environment to initialize the network \n",
    "    dummy_obs, _ = envs.reset(seed=seed)\n",
    "    if num_envs>1:\n",
    "        dummy_obs = dummy_obs[0]\n",
    "    # Initialize the network parameters\n",
    "    params = q_network.init(q_key, dummy_obs, None)\n",
    "    # Initialize the optimiser state\n",
    "    opt_state=optim.init(params)\n",
    "    # Initialize the initial train state\n",
    "    q_state = TrainState(\n",
    "        params=params,\n",
    "        target_params=params,\n",
    "        opt_state=opt_state\n",
    "    )\n",
    "    # Initialize the replay buffer\n",
    "    # We specify the size of the buffer - this is the maximum number of transitions that can be stored\n",
    "    # We specify the minimum number of transitions that must be in the buffer before we can sample\n",
    "    # We specify the number of transitions to sample at once\n",
    "    # We specify whether we will be adding sequences of transitions or individual transitions. \n",
    "    # In this case we will be adding individual transitions as we add each timestep to the buffer.\n",
    "    # We specify whether we will be adding batches of transitions or individual transitions. \n",
    "    # If we are using a vectorised environment (n_env > 1), we will add batches of transitions and \n",
    "    # specify the add batch size as the number of environments\n",
    "    buffer = fbx.make_flat_buffer(\n",
    "        max_length=buffer_size,\n",
    "        min_length=sample_batch_size,\n",
    "        sample_batch_size=sample_batch_size,\n",
    "        add_sequences=False,\n",
    "        add_batch_size=num_envs if num_envs>1 else None,\n",
    "    )\n",
    "    buffer = buffer.replace(\n",
    "        init = jax.jit(buffer.init),\n",
    "        add = jax.jit(buffer.add, donate_argnums=0),\n",
    "        sample = jax.jit(buffer.sample),\n",
    "        can_sample = jax.jit(buffer.can_sample),\n",
    "    )\n",
    "    # Create a dummy timestep to initialize the buffer\n",
    "    dummy_timestep = TimeStep(observation=dummy_obs, action=jnp.int32(0), reward=jnp.float32(0.0), discount=jnp.float32(0.0))\n",
    "    buffer_state = buffer.init(dummy_timestep)\n",
    "\n",
    "    # Create a linear schedule function for the epsilon greedy exploration\n",
    "    # linear_schedule = jax.jit(optax.polynomial_schedule(start_e, end_e, 1.0 ,exploration_fraction * total_timesteps))\n",
    "    # Faster to use custom than optax.polynomial_schedule due to jax conversions\n",
    "    def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "        slope = (end_e - start_e) / duration\n",
    "        return max(slope * t + start_e, end_e)\n",
    "\n",
    "    # Create a function to update the network\n",
    "    @jax.jit\n",
    "    def update(q_state : TrainState, batch : TimeStep):\n",
    "        \n",
    "        def loss_fn(params, target_params,  batch):\n",
    "            q_tm1 = q_network.apply(params, batch.first.observation, None)\n",
    "            a_tm1 = batch.first.action\n",
    "            r_t = batch.first.reward\n",
    "            d_t = batch.first.discount*gamma # We use first here because of the way we add transitions to the buffer\n",
    "            q_t = q_network.apply(target_params, batch.second.observation, None)\n",
    "\n",
    "            return jnp.mean(jnp.square(jax.vmap(rlax.q_learning)(q_tm1, a_tm1, r_t, d_t, q_t)))\n",
    "        \n",
    "\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(q_state.params, q_state.target_params, batch)\n",
    "        updates, new_opt_state = optim.update(grads, q_state.opt_state)  # transform grads.\n",
    "        new_params = optax.apply_updates(q_state.params, updates)  # update parameters.\n",
    "        q_state = q_state._replace(\n",
    "            params=new_params,\n",
    "            opt_state=new_opt_state\n",
    "        )\n",
    "        return loss, q_state\n",
    "\n",
    "    # Create a function to select actions from the network\n",
    "    @jax.jit\n",
    "    def action_select_fn(q_state, obs):\n",
    "        q_values = q_network.apply(q_state.params, obs, None)\n",
    "        actions = jnp.argmax(q_values, axis=-1)\n",
    "        return actions\n",
    "\n",
    "    @jax.jit\n",
    "    def perform_update(q_state, buffer_state, sample_key):\n",
    "        data = buffer.sample(buffer_state, sample_key)\n",
    "            \n",
    "        loss, q_state = update(\n",
    "            q_state,\n",
    "            data.experience\n",
    "        )\n",
    "        return loss, q_state\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run the training loop\n",
    "    print(\"Starting training...\")\n",
    "    obs, _ = envs.reset(seed=seed) # obs = np.array \n",
    "    for global_step in range(total_timesteps):\n",
    "        epsilon = linear_schedule(start_e, end_e, exploration_fraction * total_timesteps, global_step)\n",
    "        if random.random() < epsilon:\n",
    "            if num_envs > 1:\n",
    "                actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "            else:\n",
    "                actions = envs.action_space.sample()\n",
    "        else:\n",
    "            actions = action_select_fn(q_state, obs) # obs = np.array -> jnp.array\n",
    "            actions = jax.device_get(actions) # actions = jnp.array -> np.array\n",
    "\n",
    "        next_obs, rewards, terminated, truncated, infos = envs.step(actions)\n",
    "\n",
    "        if global_step % 1000 == 0:\n",
    "            print(f\"Current Training Step: {global_step}\")\n",
    "\n",
    "        # Create a timestep\n",
    "        timestep = TimeStep(observation=obs, action=actions, reward=rewards, discount = 1-np.asarray(terminated).astype(np.float32))\n",
    "\n",
    "        # # Add the timestep to the buffer\n",
    "        buffer_state = buffer.add(buffer_state, timestep)\n",
    "\n",
    "        # Update the observation\n",
    "        obs = next_obs\n",
    "        \n",
    "        # Update the network\n",
    "        loss = 0\n",
    "        if global_step > learning_starts:\n",
    "            if global_step % train_frequency == 0:\n",
    "                # Check if the buffer can sample\n",
    "                if buffer.can_sample(buffer_state):\n",
    "                    \n",
    "                    key, sample_key = jax.jit(jax.random.split)(key)\n",
    "                    loss, q_state = perform_update(q_state, buffer_state, sample_key)\n",
    "\n",
    "                if global_step % 100 == 0:\n",
    "                    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "                    print(\"Loss\", loss)\n",
    "                    \n",
    "\n",
    "            # Update the target network\n",
    "            if global_step % target_network_frequency == 0:\n",
    "                q_state = q_state._replace(\n",
    "                    target_params=optax.incremental_update(q_state.params, q_state.target_params, tau)\n",
    "                )\n",
    "\n",
    "    print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Evaluating Step : 230, episodic_return=231.0\n",
      "Evaluating Step : 598, episodic_return=368.0\n",
      "Evaluating Step : 1008, episodic_return=410.0\n",
      "Evaluating Step : 1282, episodic_return=274.0\n",
      "Evaluating Step : 1533, episodic_return=251.0\n",
      "Evaluating Step : 1747, episodic_return=214.0\n",
      "Evaluating Step : 1993, episodic_return=246.0\n",
      "Evaluating Step : 2341, episodic_return=348.0\n",
      "Evaluating Step : 2698, episodic_return=357.0\n",
      "Evaluating Step : 3097, episodic_return=399.0\n",
      "Evaluating Step : 3359, episodic_return=262.0\n",
      "Evaluating Step : 3606, episodic_return=247.0\n",
      "Evaluating Step : 3994, episodic_return=388.0\n",
      "Evaluating Step : 4228, episodic_return=234.0\n",
      "Evaluating Step : 4552, episodic_return=324.0\n",
      "Evaluating Step : 4795, episodic_return=243.0\n",
      "Evaluating Step : 5062, episodic_return=267.0\n",
      "Evaluating Step : 5355, episodic_return=293.0\n",
      "Evaluating Step : 5622, episodic_return=267.0\n",
      "Evaluating Step : 5896, episodic_return=274.0\n",
      "Evaluating Step : 6131, episodic_return=235.0\n",
      "Evaluating Step : 6392, episodic_return=261.0\n",
      "Evaluating Step : 6857, episodic_return=465.0\n",
      "Evaluating Step : 7200, episodic_return=343.0\n",
      "Evaluating Step : 7567, episodic_return=367.0\n",
      "Evaluating Step : 7808, episodic_return=241.0\n",
      "Evaluating Step : 8055, episodic_return=247.0\n",
      "Evaluating Step : 8465, episodic_return=410.0\n",
      "Evaluating Step : 8800, episodic_return=335.0\n",
      "Evaluating Step : 9045, episodic_return=245.0\n",
      "Evaluating Step : 9380, episodic_return=335.0\n",
      "Evaluating Step : 9684, episodic_return=304.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating...\")\n",
    "envs = make_env(env_id, seed)()\n",
    "obs, _ = envs.reset(seed=seed) # obs = np.array \n",
    "for global_step in range(10_000):\n",
    "    actions = action_select_fn(q_state, obs) # obs = np.array -> jnp.array\n",
    "    actions = jax.device_get(actions) # actions = jnp.array -> np.array\n",
    "\n",
    "    next_obs, rewards, terminated, truncated, infos = envs.step(actions)\n",
    "\n",
    "    # Get Episode Return Statistics\n",
    "    if \"final_info\" in infos:\n",
    "        if isinstance(infos[\"final_info\"], dict):\n",
    "            print(f\"Evaluating Step : {global_step}, episodic_return={infos['episode']['r'][0]}\")\n",
    "\n",
    "    # Update the observation\n",
    "    obs = next_obs\n",
    "\n",
    "envs.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_flash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
