{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465eeca1",
   "metadata": {},
   "source": [
    "# Quickstart: Using the Trajectory Buffer with Flashbax\n",
    "\n",
    "This guide demonstrates how to use the Trajectory Buffer for experience replay in reinforcement learning tasks. The trajectory buffer in Flashbax is a versatile tool for managing and utilizing sequences of experiences in reinforcement learning. It efficiently stores batches of trajectories while preserving their temporal ordering, making it particularly useful for scenarios involving TD-lambda errors and multi-step learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b518cf1d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "If running locally as a dev then uncomment the below 2 lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536ced4b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2caa3477",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Setup fake devices - we use this later with `jax.pmap`.\n",
    "DEVICE_COUNT_MOCK = 2\n",
    "chex.set_n_cpu_devices(DEVICE_COUNT_MOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c8a876",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import flashbax as fbx\n",
    "except ModuleNotFoundError:\n",
    "    print('installing flashbax')\n",
    "    %pip install -q flashbax\n",
    "    import flashbax as fbx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8afab",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Key Functionality of the Trajectory Buffer\n",
    "\n",
    "The trajectory buffer receives batches of trajectories, saves them while maintaining their temporal ordering, which allows sampling to return trajectories also. Similarly to the flat buffer, trajectories are saved in a first-in-first-out (FIFO) circular manner, and sampling is performed uniformly according to a desired period i.e. control over the overlap of sampled sequences. A common use case for this buffer would be if our loss uses td-lambda errors or n-step returns instead of simply the 1-step td error. Additionally, trajectories are useful for RL with any form of recurrent network.\n",
    "\n",
    "The trajectory buffer has the following key functions:\n",
    "1. `init`: Initialize the state of the buffer.\n",
    "2. `add`: Add a new batch of experience data to the buffer.\n",
    "3. `can_sample`: Check if the buffer is ready to be sampled.\n",
    "4. `sample`: Sample a batch from the buffer.\n",
    "\n",
    "Below we will go through how each of these can be used. We note the buffer is compatible with `jax.pmap` - we show how to use flashbax buffers with `jax.pmap` in this `examples.quickstart_flat_buffer.py` tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78babb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Firstly, we provide the function `trajectory_buffer.make_trajectory_buffer` which returns an instance of the `TrajectoryBuffer`. This is a `NamedTuple` containing the aforementioned `init`, `add`, `can_sample` and `sample` pure functions. We instantiate this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "285ec603",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First define hyper-parameters of the buffer.\n",
    "max_length_time_axis = 32 # Maximum length of the buffer along the time axis. \n",
    "min_length_time_axis = 16 # Minimum length across the time axis before we can sample.\n",
    "sample_batch_size = 4 # Batch size of trajectories sampled from the buffer.\n",
    "add_batch_size = 6 # Batch size of trajectories added to the buffer.\n",
    "sample_sequence_length = 8 # Sequence length of trajectories sampled from the buffer.\n",
    "add_sequence_length = 10 # Sequence length of trajectories added to the buffer.\n",
    "period = 1 # Period at which we sample trajectories from the buffer.\n",
    "\n",
    "# Instantiate the trajectory buffer, which is a NamedTuple of pure functions.\n",
    "buffer = fbx.make_trajectory_buffer(\n",
    "    max_length_time_axis=max_length_time_axis,\n",
    "    min_length_time_axis=min_length_time_axis,\n",
    "    sample_batch_size=sample_batch_size,\n",
    "    add_batch_size=add_batch_size,\n",
    "    sample_sequence_length=sample_sequence_length,\n",
    "    period=period\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e9d73",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we show how each function within the `buffer` can be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bfb0fe",
   "metadata": {},
   "source": [
    "## Initialize the Buffer State\n",
    "\n",
    "To demonstrate how to use the buffer, we'll start by initializing its state using the `init` function. This requires a unit of experience data, which is used to infer the structure of the experience that will be added later. For this example, we create a fake timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b469c1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1695739304.514052 1494773 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/e.toledo/miniconda3/envs/flashbax/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1695739304.514136 1494773 pjrt_api.cc:67] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1695739304.514140 1494773 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.\n",
      "I0000 00:00:1695739308.103153 1494773 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n"
     ]
    }
   ],
   "source": [
    "fake_timestep = {\"obs\": jnp.array([5, 4]), \"reward\": jnp.array(1.0)} \n",
    "state = buffer.init(fake_timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb20ef",
   "metadata": {},
   "source": [
    "## Adding Experience to the Buffer\n",
    "To fill the buffer above its minimum length, we use the `add` function. The function expects batches of sequences of experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc089ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(0)\n",
    "# Now fill the buffer above its minimum length using the `add` function.\n",
    "# The add function expects batches of trajectories.\n",
    "# Thus, we create a fake batch of trajectories by broadcasting the `fake_timestep`.\n",
    "broadcast_fn = lambda x: jnp.broadcast_to(x, (add_batch_size, add_sequence_length, *x.shape))\n",
    "fake_batch_sequence = jax.tree.map(broadcast_fn, fake_timestep)\n",
    "state = buffer.add(state, fake_batch_sequence)\n",
    "assert buffer.can_sample(state) == False  # After one batch the buffer is not yet full.\n",
    "state = buffer.add(state, fake_batch_sequence)\n",
    "assert buffer.can_sample(state)  # Now the buffer is ready to be sampled. i.e it is filled above min_length_time_axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912298f0",
   "metadata": {},
   "source": [
    "## Sampling from the Buffer\n",
    "To sample from the buffer, we use the `sample` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971acbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(0)  # Setup source of randomness\n",
    "# Sample from the buffer. This returns a batch of **sequences** of data with the same structure as \n",
    "# `fake_timestep`.\n",
    "batch = buffer.sample(state, rng_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28570644",
   "metadata": {},
   "source": [
    "By inspecting the batch object, you can see that it matches the structure of the `fake_timestep` but with an extra leading batch dimension and a sequence dimension of `sample_sequence_length` representing a sequence of timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a36aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['obs', 'reward'])\n",
      "(4, 8)\n"
     ]
    }
   ],
   "source": [
    "print(batch.experience.keys()) # prints dict_keys(['obs', 'reward'])\n",
    "print(batch.experience['reward'].shape) # prints (4,8) = (sample_batch_size, sample_sequence_length, *fake_transition['reward'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ce4d9",
   "metadata": {},
   "source": [
    "## Buffer State and Structure\n",
    "\n",
    "By inspecting the buffer state we see that it contains \n",
    "\n",
    "- `experience` which is a pytree matching the structure of `fake_timestep` but with extra axes of size `add_batch_size` and `max_length`\n",
    "- a current index that keeps track of where along the time dimension in the buffer experience should be added\n",
    "- a `is_full` boolean array which notes if the buffer has been filled above the `max_length_time_axis`, after which new added experience starts overwriting old experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26863454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['experience', 'current_index', 'is_full'])\n",
      "dict_keys(['obs', 'reward'])\n",
      "(6, 32, 2)\n"
     ]
    }
   ],
   "source": [
    "print(state.__dict__.keys())\n",
    "print(state.experience.keys())\n",
    "# prints (6,32,2) = (add_batch_size, max_length_time_axis, *fake_timestep['obs'].shape)\n",
    "print(state.experience['obs'].shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602a215",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To understand the specifics of how the trajectory buffer works we recommend inspecting the above objects returned from each of the key buffer functions, while looking at the code and documentation in the `flashbax.buffer.trajectory_buffer.py` file. For example, by inspecting the `batch` object we see that it is a batch of **sequences** of experience data (i.e. a batch of trajectories) with the same structure as `fake_timestep`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "541ce614",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['obs', 'reward'])\n",
      "(4, 8)\n"
     ]
    }
   ],
   "source": [
    "print(batch.experience.keys()) # prints dict_keys(['obs', 'reward'])\n",
    "# prints (4, 8) = (sample_batch_size, sample_sequence_length, *fake_timestep['reward'].shape)\n",
    "print(batch.experience['reward'].shape) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
